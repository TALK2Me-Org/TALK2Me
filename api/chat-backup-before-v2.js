// TALK2Me Chat API - Vercel Serverless Function v4.0 - Chat Completions z Streamingiem
import { createClient } from '@supabase/supabase-js'
import { Groq } from 'groq-sdk'
import OpenAI from 'openai'

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL
const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY

// Cache dla promptu Assistant API
export const promptCache = {
  prompt: null,
  timestamp: 0,
  source: 'none'
}

// Vercel nie wspiera prawdziwego streamingu w serverless functions
// Ale moÅ¼emy symulowaÄ‡ streaming przez chunked responses
export default async function handler(req, res) {
  // CORS headers
  res.setHeader('Access-Control-Allow-Origin', '*')
  res.setHeader('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type, Authorization')
  
  if (req.method === 'OPTIONS') {
    return res.status(200).end()
  }

  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' })
  }

  try {
    const { message, userContext } = req.body
    const authHeader = req.headers.authorization
    
    if (!message) {
      return res.status(400).json({ error: 'Message is required' })
    }

    const supabase = createClient(supabaseUrl, supabaseServiceKey)
    
    // SprawdÅº czy user jest zalogowany
    let userId = null
    if (authHeader && authHeader.startsWith('Bearer ')) {
      const token = authHeader.split(' ')[1]
      const { data: { user } } = await supabase.auth.getUser(token)
      userId = user?.id
    }

    // Pobierz konfiguracjÄ™ AI
    const { data: config } = await supabase
      .from('app_config')
      .select('config_key, config_value')
    
    const configMap = {}
    config?.forEach(item => {
      configMap[item.config_key] = item.config_value
    })

    const userMessage = message
    const activeModel = configMap.active_model || 'openai'
    
    // Przygotuj streaming response
    res.setHeader('Content-Type', 'text/event-stream')
    res.setHeader('Cache-Control', 'no-cache')
    res.setHeader('Connection', 'keep-alive')
    res.setHeader('X-Accel-Buffering', 'no') // WyÅ‚Ä…cz buforowanie nginx
    
    let fullResponse = ''
    let streamSuccess = false

    // 1. PrÃ³buj OpenAI z streamingiem
    const openaiKey = configMap.openai_api_key
    const assistantId = configMap.assistant_id
    
    if (activeModel === 'openai' && openaiKey) {
      try {
        const openai = new OpenAI({ apiKey: openaiKey })
        
        // Pobierz prompt z cache lub Assistant API
        let systemPrompt = 'You are a helpful AI assistant.'
        
        // SprawdÅº cache w pamiÄ™ci
        if (promptCache.prompt && Date.now() - promptCache.timestamp < 3600000) { // 1h
          systemPrompt = promptCache.prompt
          console.log('ðŸ“‹ UÅ¼ywam promptu z cache (source: ' + promptCache.source + ')')
        } else if (assistantId) {
          // Pobierz Å›wieÅ¼y prompt z Assistant API
          try {
            console.log('ðŸ“¥ Pobieram prompt z Assistant API...')
            const assistant = await openai.beta.assistants.retrieve(assistantId)
            systemPrompt = assistant.instructions || systemPrompt
            
            // Zapisz do cache
            promptCache.prompt = systemPrompt
            promptCache.timestamp = Date.now()
            promptCache.source = 'Assistant API'
            
            console.log('âœ… Prompt pobrany i zapisany do cache (dÅ‚ugoÅ›Ä‡: ' + systemPrompt.length + ' znakÃ³w)')
          } catch (err) {
            console.log('âš ï¸ Nie udaÅ‚o siÄ™ pobraÄ‡ promptu z Assistant API:', err.message)
          }
        }
        
        // Wybierz model z konfiguracji (domyÅ›lnie gpt-4o)
        const modelName = configMap.openai_model || 'gpt-4o';
        console.log('ðŸ¤– UÅ¼ywam modelu:', modelName);
        console.log('ðŸ“Š PeÅ‚na konfiguracja:', {
          model: modelName,
          temperature: configMap.temperature,
          max_tokens: configMap.max_tokens,
          promptSource: promptCache.source,
          promptLength: systemPrompt.length
        });
        
        // Stream response
        let stream;
        try {
          stream = await openai.chat.completions.create({
            model: modelName,
            messages: [
              { role: "system", content: systemPrompt },
              { role: "user", content: userMessage }
            ],
            temperature: parseFloat(configMap.temperature) || 0.7,
            max_tokens: parseInt(configMap.max_tokens) || 1000,
            stream: true
          })
        } catch (modelError) {
          // JeÅ›li model nie istnieje, sprÃ³buj z gpt-4o
          if (modelError.message?.includes('model') || modelError.status === 404) {
            console.log(`âš ï¸ Model ${modelName} niedostÄ™pny, uÅ¼ywam gpt-4o`)
            stream = await openai.chat.completions.create({
              model: 'gpt-4o',
              messages: [
                { role: "system", content: systemPrompt },
                { role: "user", content: userMessage }
              ],
              temperature: parseFloat(configMap.temperature) || 0.7,
              max_tokens: parseInt(configMap.max_tokens) || 1000,
              stream: true
            })
          } else {
            throw modelError
          }
        }
        
        // Stream chunks do klienta
        for await (const chunk of stream) {
          const content = chunk.choices[0]?.delta?.content || ''
          if (content) {
            fullResponse += content
            // WyÅ›lij chunk do klienta
            res.write(`data: ${JSON.stringify({ content })}\n\n`)
          }
        }
        
        streamSuccess = true
        console.log('âœ… OpenAI streaming completed')
        
      } catch (error) {
        console.log('âŒ OpenAI streaming failed:', error.message)
        res.write(`data: ${JSON.stringify({ error: 'OpenAI failed, trying fallback...' })}\n\n`)
      }
    }

    // 2. Fallback: Groq (bez streamingu - wyÅ›lij caÅ‚oÅ›Ä‡ na raz)
    const groqEnabled = configMap.groq_enabled === 'true'
    if (!streamSuccess && groqEnabled && configMap.groq_api_key) {
      try {
        const groq = new Groq({ apiKey: configMap.groq_api_key })
        
        const completion = await groq.chat.completions.create({
          messages: [
            { role: 'user', content: userMessage }
          ],
          model: 'llama3-8b-8192',
          temperature: parseFloat(configMap.temperature) || 0.7,
          max_tokens: parseInt(configMap.max_tokens) || 1000
        })

        fullResponse = completion.choices[0].message.content
        // WyÅ›lij caÅ‚Ä… odpowiedÅº jako jeden chunk
        res.write(`data: ${JSON.stringify({ content: fullResponse })}\n\n`)
        streamSuccess = true
        console.log('âœ… Groq response successful (fallback)')
        
      } catch (error) {
        console.log('âŒ Groq failed:', error.message)
      }
    }

    // 3. JeÅ›li nic nie zadziaÅ‚aÅ‚o
    if (!streamSuccess) {
      res.write(`data: ${JSON.stringify({ 
        error: 'Nie udaÅ‚o siÄ™ uzyskaÄ‡ odpowiedzi od AI. SprawdÅº konfiguracjÄ™ kluczy API w panelu administracyjnym.' 
      })}\n\n`)
    }

    // Zapisz historiÄ™ jeÅ›li user zalogowany i mamy odpowiedÅº
    if (userId && fullResponse) {
      await supabase
        .from('chat_history')
        .insert({
          user_id: userId,
          message: message,
          response: fullResponse,
          ai_model: activeModel
        })
    }

    // ZakoÅ„cz stream
    res.write(`data: [DONE]\n\n`)
    res.end()

  } catch (error) {
    console.error('Chat API error:', error)
    // W przypadku bÅ‚Ä™du teÅ¼ musimy zakoÅ„czyÄ‡ jako event stream
    res.write(`data: ${JSON.stringify({ 
      error: 'Internal server error', 
      details: error.message 
    })}\n\n`)
    res.write(`data: [DONE]\n\n`)
    res.end()
  }
}